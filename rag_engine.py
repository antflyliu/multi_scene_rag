#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@File    : rag_engine.py
@Author  : Kevin
@Date    : 2025/10/26
@Description : å¤šåœºæ™¯RAGå¼•æ“.
@Version : 1.0
"""

import os
import shutil
import time
import json
import hashlib
from pathlib import Path
import re
from typing import Any, Callable
from llama_index.core import (
    Settings,
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage
)
from llama_index.core.callbacks import CallbackManager, TokenCountingHandler
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.embeddings.dashscope import (
    DashScopeEmbedding,
    DashScopeTextEmbeddingModels,
    DashScopeTextEmbeddingType,
)
from classifier import classify_scene
from config_store import (
    LLM_VENDOR_BASE_URLS,
    get_api_key,
    get_base_url,
    get_default_scene_key,
    get_embedding_device,
    get_embedding_model,
    get_embedding_provider,
    get_embedding_source,
    get_llm_vendor,
    get_model_name,
    get_scenes,
)

RebuildProgressCallback = Callable[[dict[str, Any]], None]
REBUILD_MANIFEST_PATH = Path("./storage/rebuild_manifest.json")
SCENE_CATALOG_PATH = Path("./storage/scene_catalog.json")

# å…¨å±€é»˜è®¤æ„å›¾åŒ¹é…æ¨¡å¼â€”â€”æ‰€æœ‰åœºæ™¯å…¬ç”¨ï¼Œä¼˜å…ˆçº§ä½äºåœºæ™¯çº§è‡ªå®šä¹‰è¯å…¸ã€‚
_DEFAULT_COUNT_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"(å‡ |å¤šå°‘).*(æœ¬|ä¸ª|æ¡|ä»½|ç¯‡|æ–‡ä»¶|æ–‡æ¡£|ä¹¦|è§„åˆ™|æ”¿ç­–|åˆ¶åº¦|æµç¨‹|é—®é¢˜|é¡¹ç›®|ç§|ç±»)", re.IGNORECASE),
    re.compile(r"(æœ‰|å…±|æ€»å…±).*(å‡ |å¤šå°‘)", re.IGNORECASE),
    re.compile(r"(æ•°é‡|æ€»æ•°|æ€»è®¡|å…±è®¡)", re.IGNORECASE),
    re.compile(r"(æœ‰å‡ æœ¬|æœ‰å‡ ä¸ª|æœ‰å‡ æ¡|æœ‰å‡ ä»½|æœ‰å‡ ç±»|æœ‰å‡ ç§)", re.IGNORECASE),
]
_DEFAULT_LIST_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"(åˆ—å‡º|ç½—åˆ—|æšä¸¾|æ¸…å•|ç›®å½•|æœ‰å“ªäº›|éƒ½æœ‰ä»€ä¹ˆ|å…¨éƒ¨|æ‰€æœ‰).*(ä¹¦|æ–‡æ¡£|æ–‡ä»¶|èµ„æ–™|å†…å®¹|è§„åˆ™|æ”¿ç­–|æµç¨‹|é—®é¢˜|é¡¹ç›®)?", re.IGNORECASE),
    re.compile(r"(æœ‰å“ªäº›|åŒ…å«å“ªäº›|æ¶µç›–å“ªäº›).*(æ–‡æ¡£|èµ„æ–™|å†…å®¹|ä¹¦|è§„åˆ™|æ”¿ç­–|åˆ¶åº¦)?", re.IGNORECASE),
]

# åœºæ™¯çº§è‡ªå®šä¹‰æ„å›¾è¯å…¸ï¼šscene_key -> {"count": [...], "list": [...]}
# config.py ä¸­å¯é€šè¿‡ SCENE_INTENT_OVERRIDES è¦†ç›–ï¼ˆå¯é€‰ï¼‰ã€‚
_BUILTIN_SCENE_INTENT_OVERRIDES: dict[str, dict[str, list[re.Pattern[str]]]] = {
    "book": {
        "count": [re.compile(r"(å‡ æœ¬ä¹¦|å¤šå°‘æœ¬ä¹¦|ä¹¦çš„æ•°é‡|æœ‰å‡ æœ¬ä¹¦|å…±.*æœ¬ä¹¦)", re.IGNORECASE)],
        "list": [re.compile(r"(åˆ—å‡º.*ä¹¦|ä¹¦.*æ¸…å•|æœ‰å“ªäº›ä¹¦|æ‰€æœ‰ä¹¦ç±|ä¹¦ç›®|ä¹¦ç±.*ç›®å½•)", re.IGNORECASE)],
    },
    "hr": {
        "count": [re.compile(r"(å‡ æ¡.*è§„å®š|å‡ ç§.*å‡|å‡ ç±».*ç¦åˆ©|å‡ ä¸ª.*æµç¨‹)", re.IGNORECASE)],
        "list": [re.compile(r"(åˆ—å‡º.*æ”¿ç­–|æœ‰å“ªäº›.*åˆ¶åº¦|æ‰€æœ‰.*æµç¨‹|å‡æœŸ.*ç§ç±»)", re.IGNORECASE)],
    },
    "finance": {
        "count": [re.compile(r"(å‡ æ¡.*æŠ¥é”€|å‡ ç±».*è´¹ç”¨|å‡ ç§.*å‘ç¥¨|å‡ ä¸ª.*è§„åˆ™)", re.IGNORECASE)],
        "list": [re.compile(r"(åˆ—å‡º.*æŠ¥é”€|æœ‰å“ªäº›.*è´¹ç”¨|æ‰€æœ‰.*å‘ç¥¨ç±»å‹)", re.IGNORECASE)],
    },
    "it": {
        "count": [re.compile(r"(å‡ ç±».*é—®é¢˜|å‡ ç§.*æ•…éšœ|å‡ ä¸ª.*è´¦å·|å‡ å¥—.*ç³»ç»Ÿ)", re.IGNORECASE)],
        "list": [re.compile(r"(åˆ—å‡º.*è½¯ä»¶|æœ‰å“ªäº›.*ç³»ç»Ÿ|æ‰€æœ‰.*è´¦å·|æ”¯æŒ.*å“ªäº›.*è®¾å¤‡)", re.IGNORECASE)],
    },
}


def _compile_scene_intent_overrides() -> dict[str, dict[str, list[re.Pattern[str]]]]:
    """åˆå¹¶å†…ç½®åœºæ™¯è¯å…¸ä¸ config ä¸­çš„å¯é€‰æ‰©å±•ã€‚"""
    try:
        from config import SCENE_INTENT_OVERRIDES as _extra  # type: ignore[import]
        merged: dict[str, dict[str, list[re.Pattern[str]]]] = {}
        all_keys = set(_BUILTIN_SCENE_INTENT_OVERRIDES) | set(_extra)
        for key in all_keys:
            merged[key] = {}
            for intent in ("count", "list"):
                builtin_pats = _BUILTIN_SCENE_INTENT_OVERRIDES.get(key, {}).get(intent, [])
                extra_pats = _extra.get(key, {}).get(intent, [])
                merged[key][intent] = builtin_pats + [
                    re.compile(p, re.IGNORECASE) if isinstance(p, str) else p
                    for p in extra_pats
                ]
        return merged
    except (ImportError, AttributeError):
        return _BUILTIN_SCENE_INTENT_OVERRIDES


SCENE_INTENT_OVERRIDES: dict[str, dict[str, list[re.Pattern[str]]]] = _compile_scene_intent_overrides()


def configure_runtime_models(runtime_mode: str = "web") -> None:
    """æŒ‰è¿è¡Œæ—¶é…ç½®è®¾ç½®å…¨å±€ LLM ä¸ Embedding æ¨¡å‹ã€‚

    å½“å‰ç­–ç•¥ï¼š
    - LLM: æŒ‰å‚å•†è·¯ç”±ï¼ˆdashscope èµ°åŸç”Ÿ SDKï¼Œå…¶å®ƒèµ° OpenAI å…¼å®¹æ¥å£ï¼‰
    - Embedding: DashScopeï¼ˆä¿æŒåŸæœ‰å‘é‡æ„å»ºé“¾è·¯ç¨³å®šï¼‰
    """
    api_key = get_api_key(runtime_mode=runtime_mode) or os.getenv("DASHSCOPE_API_KEY", "")
    llm_vendor = get_llm_vendor(runtime_mode=runtime_mode)
    base_url = get_base_url(runtime_mode=runtime_mode)
    model_name = get_model_name(runtime_mode=runtime_mode)
    embedding_provider = get_embedding_provider(runtime_mode=runtime_mode)
    embedding_model = get_embedding_model(runtime_mode=runtime_mode)
    embedding_source = get_embedding_source(runtime_mode=runtime_mode)
    embedding_device = _resolve_embedding_device(get_embedding_device(runtime_mode=runtime_mode))
    if not api_key:
        raise ValueError("API_KEY æœªé…ç½®ï¼Œè¯·å…ˆåœ¨è®¾ç½®é¡µä¿å­˜ API Keyã€‚")

    openai_compatible_vendors = {vendor for vendor in LLM_VENDOR_BASE_URLS.keys() if vendor != "dashscope"}

    # max_tokens éœ€å°äº context_windowï¼Œå¦åˆ™ prompt_helper è®¡ç®— available_context ä¼šä¸ºè´Ÿå¯¼è‡´ ValueErrorã€‚
    # QWEN_MAX context=8192ï¼Œéœ€é¢„ç•™ç©ºé—´ç»™ RAG æ£€ç´¢å—+æ¨¡æ¿ï¼Œ4096 æ—¢èƒ½ä¿è¯é•¿ç­”æ¡ˆå®Œæ•´åˆé¿å… -125 ç±»é”™è¯¯ã€‚
    _max_tokens = 4096

    if llm_vendor == "dashscope":
        Settings.llm = DashScope(
            api_key=api_key,
            model_name=model_name or DashScopeGenerationModels.QWEN_MAX,
            max_tokens=_max_tokens,
        )
    elif llm_vendor in openai_compatible_vendors:
        Settings.llm = LlamaOpenAI(
            api_key=api_key,
            api_base=base_url,
            model=model_name,
            max_tokens=_max_tokens,
        )
    else:
        raise ValueError(
            f"ä¸æ”¯æŒçš„ llm_vendor: {llm_vendor}ã€‚"
            "è¯·ä½¿ç”¨ dashscopeã€openaiã€claudeã€geminiã€glmã€kimi æˆ– customã€‚"
        )
    if embedding_provider == "dashscope":
        Settings.embed_model = DashScopeEmbedding(
            api_key=api_key,
            model_name=embedding_model or DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1,
            text_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,
        )
    elif embedding_provider == "openai":
        Settings.embed_model = OpenAIEmbedding(
            api_key=api_key,
            api_base=base_url,
            model=embedding_model,
        )
    elif embedding_provider == "local":
        local_model_path_or_id = _resolve_local_embedding_model(embedding_source, embedding_model)
        Settings.embed_model = HuggingFaceEmbedding(
            model_name=local_model_path_or_id,
            device=embedding_device,
        )
    else:
        raise ValueError(
            f"ä¸æ”¯æŒçš„ embedding_provider: {embedding_provider}ã€‚"
            "è¯·ä½¿ç”¨ dashscopeã€openai æˆ– localã€‚"
        )


def _resolve_embedding_device(config_device: str) -> str:
    normalized = (config_device or "cpu").lower()
    if normalized not in {"cpu", "cuda"}:
        return "cpu"
    if normalized == "cuda":
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
            print("âš ï¸ embedding_device=cuda ä½†æœªæ£€æµ‹åˆ°å¯ç”¨ CUDAï¼Œè‡ªåŠ¨å›é€€åˆ° cpuã€‚")
            return "cpu"
        except Exception:
            print("âš ï¸ å½“å‰ç¯å¢ƒç¼ºå°‘ torch/CUDA æ”¯æŒï¼Œembedding_device è‡ªåŠ¨å›é€€åˆ° cpuã€‚")
            return "cpu"
    return "cpu"


def _resolve_local_embedding_model(source: str, embedding_model: str) -> str:
    normalized_source = (source or "huggingface").lower()
    model_value = (embedding_model or "").strip()
    if not model_value:
        raise ValueError("embedding_model ä¸èƒ½ä¸ºç©ºã€‚")

    if normalized_source == "huggingface":
        return _resolve_huggingface_model_path(model_value)

    if normalized_source == "local":
        model_path = Path(model_value)
        if not model_path.exists():
            raise FileNotFoundError(f"æœ¬åœ° embedding æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_value}")
        return str(model_path)

    if normalized_source == "modelscope":
        try:
            from modelscope.hub.snapshot_download import snapshot_download
        except Exception as exc:
            raise RuntimeError(
                "ä½¿ç”¨ modelscope éœ€è¦å…ˆå®‰è£… modelscopeï¼špip install modelscope"
            ) from exc
        cache_dir = Path("./embedding/modelscope")
        cache_dir.mkdir(parents=True, exist_ok=True)
        downloaded_path = snapshot_download(model_id=model_value, cache_dir=str(cache_dir))
        return str(downloaded_path)

    raise ValueError(
        f"ä¸æ”¯æŒçš„ embedding_source: {normalized_source}ã€‚"
        "è¯·ä½¿ç”¨ huggingfaceã€modelscope æˆ– localã€‚"
    )


def _sanitize_model_id(model_id: str) -> str:
    """å°†æ¨¡å‹ ID è½¬ä¸ºå¯ä½œä¸ºç›®å½•åçš„å®‰å…¨å­—ç¬¦ä¸²ã€‚"""
    return re.sub(r"[^\w\-.]+", "__", model_id)


def _resolve_huggingface_model_path(model_id: str) -> str:
    """å°† HuggingFace æ¨¡å‹ä¸‹è½½/ç¼“å­˜åˆ° embedding/huggingfaceã€‚"""
    target_root = Path("./embedding/huggingface")
    target_root.mkdir(parents=True, exist_ok=True)
    model_dir = target_root / _sanitize_model_id(model_id)

    # è‹¥ç›®å½•å·²å­˜åœ¨å¹¶ä¸”éç©ºï¼Œè§†ä¸ºå·²ç¼“å­˜ï¼Œç›´æ¥å¤ç”¨ã€‚
    if model_dir.exists() and any(model_dir.iterdir()):
        return str(model_dir)

    try:
        from huggingface_hub import snapshot_download
    except Exception as exc:
        raise RuntimeError(
            "ä½¿ç”¨ huggingface ä½œä¸º embedding_source éœ€è¦å®‰è£… huggingface_hubã€‚"
        ) from exc

    snapshot_download(
        repo_id=model_id,
        local_dir=str(model_dir),
        local_dir_use_symlinks=False,
    )
    return str(model_dir)

class MultiSceneRAG:
    def __init__(self, runtime_mode: str = "web", eager_init: bool = True):
        self.runtime_mode = runtime_mode
        configure_runtime_models(runtime_mode=runtime_mode)
        self._token_counter: TokenCountingHandler | None = None
        try:
            self._token_counter = TokenCountingHandler()
            Settings.callback_manager = CallbackManager([self._token_counter])
        except Exception:
            # å…¼å®¹ä¸åŒ llama-index ç‰ˆæœ¬ï¼›ä¸å¯ç”¨æ—¶å›é€€ä¸ºä¸ç»Ÿè®¡ tokenã€‚
            self._token_counter = None
        self.scenes = get_scenes()
        self.indices = {}
        self._eager_init = bool(eager_init)
        if self._eager_init:
            self._init_indices()

    @staticmethod
    def _read_rebuild_manifest() -> dict[str, Any]:
        if not REBUILD_MANIFEST_PATH.exists():
            return {"scenes": {}}
        try:
            data = json.loads(REBUILD_MANIFEST_PATH.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                scenes_data = data.get("scenes", {})
                if isinstance(scenes_data, dict):
                    return {"scenes": scenes_data}
        except Exception:
            pass
        return {"scenes": {}}

    @staticmethod
    def _write_rebuild_manifest(data: dict[str, Any]) -> None:
        REBUILD_MANIFEST_PATH.parent.mkdir(parents=True, exist_ok=True)
        REBUILD_MANIFEST_PATH.write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

    @staticmethod
    def _read_scene_catalog() -> dict[str, Any]:
        if not SCENE_CATALOG_PATH.exists():
            return {"scenes": {}}
        try:
            data = json.loads(SCENE_CATALOG_PATH.read_text(encoding="utf-8"))
            if isinstance(data, dict) and isinstance(data.get("scenes"), dict):
                return data
        except Exception:
            pass
        return {"scenes": {}}

    @staticmethod
    def _write_scene_catalog(catalog: dict[str, Any]) -> None:
        SCENE_CATALOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        SCENE_CATALOG_PATH.write_text(
            json.dumps(catalog, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

    def _get_embedding_signature(self) -> dict[str, str]:
        return {
            "embedding_provider": get_embedding_provider(runtime_mode=self.runtime_mode),
            "embedding_model": get_embedding_model(runtime_mode=self.runtime_mode),
            "embedding_source": get_embedding_source(runtime_mode=self.runtime_mode),
            "embedding_device": get_embedding_device(runtime_mode=self.runtime_mode),
        }

    @staticmethod
    def _build_data_signature(data_path: str) -> dict[str, Any]:
        root = Path(data_path)
        if not root.exists():
            return {"exists": False, "files": [], "file_count": 0, "total_size": 0}
        file_entries: list[dict[str, Any]] = []
        total_size = 0
        for file_path in sorted([p for p in root.rglob("*") if p.is_file()]):
            stat = file_path.stat()
            relative_path = file_path.relative_to(root).as_posix()
            size = int(stat.st_size)
            mtime_ns = int(stat.st_mtime_ns)
            total_size += size
            file_entries.append(
                {
                    "path": relative_path,
                    "size": size,
                    "mtime_ns": mtime_ns,
                }
            )
        return {
            "exists": True,
            "files": file_entries,
            "file_count": len(file_entries),
            "total_size": total_size,
        }

    def _scene_rebuild_signature(self, scene_key: str, scene_info: dict[str, Any]) -> str:
        signature_payload = {
            "scene_key": scene_key,
            "scene_config": {
                "name": scene_info.get("name", ""),
                "keywords": scene_info.get("keywords", []),
                "path": scene_info.get("path", ""),
            },
            "embedding": self._get_embedding_signature(),
            "data": self._build_data_signature(str(scene_info.get("path", ""))),
        }
        raw = json.dumps(signature_payload, ensure_ascii=False, sort_keys=True)
        return hashlib.sha256(raw.encode("utf-8")).hexdigest()

    @staticmethod
    def _scene_file_entries(data_path: str) -> list[dict[str, Any]]:
        root = Path(data_path)
        if not root.exists():
            return []
        entries: list[dict[str, Any]] = []
        for file_path in sorted([p for p in root.rglob("*") if p.is_file()]):
            stat = file_path.stat()
            entries.append(
                {
                    "file_name": file_path.name,
                    "file_path": str(file_path.resolve()),
                    "relative_path": file_path.relative_to(root).as_posix(),
                    "size": int(stat.st_size),
                    "mtime_ns": int(stat.st_mtime_ns),
                }
            )
        return entries

    def _update_scene_catalog(self, scene_key: str) -> None:
        scene_info = self.scenes.get(scene_key)
        if not scene_info:
            return
        data_path = str(scene_info.get("path", ""))
        files = self._scene_file_entries(data_path)
        data_signature = self._scene_rebuild_signature(scene_key, scene_info)
        catalog = self._read_scene_catalog()
        scenes_data = catalog.setdefault("scenes", {})
        scenes_data[scene_key] = {
            "scene_name": str(scene_info.get("name", scene_key)),
            "data_path": data_path,
            "data_signature": data_signature,
            "updated_at": int(time.time()),
            "files": files,
        }
        self._write_scene_catalog(catalog)

    def _get_scene_catalog_files(self, scene_key: str) -> list[dict[str, Any]]:
        scene_info = self.scenes.get(scene_key, {})
        current_signature = self._scene_rebuild_signature(scene_key, scene_info)
        catalog = self._read_scene_catalog()
        scene_data = catalog.get("scenes", {}).get(scene_key, {})
        if (
            isinstance(scene_data, dict)
            and scene_data.get("data_signature") == current_signature
            and isinstance(scene_data.get("files"), list)
        ):
            return [f for f in scene_data.get("files", []) if isinstance(f, dict)]
        # ç›®å½•æˆ–é…ç½®å˜åŒ–åè‡ªåŠ¨å›æºåˆ·æ–°ï¼Œé¿å…ç»Ÿè®¡ç­”æ¡ˆæ»åã€‚
        self._update_scene_catalog(scene_key)
        refreshed = self._read_scene_catalog().get("scenes", {}).get(scene_key, {})
        files = refreshed.get("files", []) if isinstance(refreshed, dict) else []
        return [f for f in files if isinstance(f, dict)]

    def _detect_query_intent(self, user_query: str, scene_key: str = "") -> str:
        """è¯†åˆ«æŸ¥è¯¢æ„å›¾ï¼šcount / list / qaã€‚

        ä¼˜å…ˆåŒ¹é…åœºæ™¯çº§è¯å…¸ï¼Œå…¶æ¬¡ä½¿ç”¨å…¨å±€é»˜è®¤è¯å…¸ã€‚
        """
        text = (user_query or "").strip()
        scene_overrides = SCENE_INTENT_OVERRIDES.get(scene_key, {})
        for pattern in scene_overrides.get("count", []):
            if pattern.search(text):
                return "count"
        for pattern in scene_overrides.get("list", []):
            if pattern.search(text):
                return "list"
        for pattern in _DEFAULT_COUNT_PATTERNS:
            if pattern.search(text):
                return "count"
        for pattern in _DEFAULT_LIST_PATTERNS:
            if pattern.search(text):
                return "list"
        return "qa"

    def _build_structured_answer(self, scene_key: str, user_query: str) -> str | None:
        intent = self._detect_query_intent(user_query, scene_key=scene_key)
        if intent not in {"count", "list"}:
            return None
        scene_info = self.scenes.get(scene_key)
        if not scene_info:
            return None
        files = self._get_scene_catalog_files(scene_key)
        scene_name = str(scene_info.get("name", scene_key))
        count = len(files)
        if intent == "count":
            sample_names = [str(item.get("file_name", "")) for item in files[:3]]
            if count == 0:
                return f"å½“å‰åœºæ™¯â€œ{scene_name}â€æš‚æ— æ–‡æ¡£ã€‚"
            if sample_names:
                return (
                    f"å½“å‰åœºæ™¯â€œ{scene_name}â€å…±æœ‰ {count} ä»½æ–‡æ¡£ã€‚"
                    f"ä¾‹å¦‚ï¼š{'; '.join(sample_names)}ã€‚"
                )
            return f"å½“å‰åœºæ™¯â€œ{scene_name}â€å…±æœ‰ {count} ä»½æ–‡æ¡£ã€‚"

        if count == 0:
            return f"å½“å‰åœºæ™¯â€œ{scene_name}â€æš‚æ— å¯åˆ—å‡ºçš„æ–‡æ¡£ã€‚"
        lines = [f"å½“å‰åœºæ™¯â€œ{scene_name}â€å…±æœ‰ {count} ä»½æ–‡æ¡£ï¼š"]
        for idx, item in enumerate(files, start=1):
            lines.append(f"{idx}. {item.get('file_name', 'æœªå‘½åæ–‡æ¡£')}")
            if idx >= 30:
                lines.append(f"... å…¶ä½™ {count - 30} ä»½æ–‡æ¡£å·²çœç•¥ã€‚")
                break
        return "\n".join(lines)

    def _resolve_scene_for_structured_query(self, routed_scene: str, user_query: str) -> str:
        intent = self._detect_query_intent(user_query, scene_key=routed_scene)
        if intent not in {"count", "list"}:
            return routed_scene
        text = (user_query or "").strip().lower()
        matched_scenes: list[str] = []
        for scene_key, scene_info in self.scenes.items():
            name = str(scene_info.get("name", "")).strip().lower()
            if scene_key.lower() in text or (name and name in text):
                matched_scenes.append(scene_key)
        if len(matched_scenes) == 1:
            return matched_scenes[0]
        if "book" in self.scenes and ("ä¹¦" in user_query or "epub" in text):
            return "book"
        return routed_scene

    def _collect_changed_scenes(self) -> tuple[list[str], list[dict[str, str]]]:
        manifest = self._read_rebuild_manifest()
        scene_signatures = manifest.get("scenes", {})
        changed: list[str] = []
        reasons: list[dict[str, str]] = []
        for scene_key, scene_info in self.scenes.items():
            current_signature = self._scene_rebuild_signature(scene_key, scene_info)
            previous_signature = str(scene_signatures.get(scene_key, ""))
            persist_dir = Path(f"./storage/{scene_key}")
            docstore_path = persist_dir / "docstore.json"
            if not previous_signature:
                changed.append(scene_key)
                reasons.append({"scene_key": scene_key, "reason": "é¦–æ¬¡æ„å»ºæˆ–ç¼ºå°‘å†å²ç­¾å"})
                continue
            if not persist_dir.exists() or not docstore_path.exists():
                changed.append(scene_key)
                reasons.append({"scene_key": scene_key, "reason": "ç´¢å¼•æ–‡ä»¶ç¼ºå¤±"})
                continue
            if current_signature != previous_signature:
                changed.append(scene_key)
                reasons.append({"scene_key": scene_key, "reason": "æ•°æ®æˆ–é…ç½®å‘ç”Ÿå˜åŒ–"})
        return changed, reasons

    def _update_scene_manifest(self, scene_key: str) -> None:
        scene_info = self.scenes.get(scene_key)
        if not scene_info:
            return
        manifest = self._read_rebuild_manifest()
        scenes_data = manifest.setdefault("scenes", {})
        scenes_data[scene_key] = self._scene_rebuild_signature(scene_key, scene_info)
        self._write_rebuild_manifest(manifest)
        self._update_scene_catalog(scene_key)

    def _init_indices(self):
        """ä¸ºæ¯ä¸ªåœºæ™¯æ„å»ºæˆ–åŠ è½½å‘é‡ç´¢å¼•"""
        for scene, info in self.scenes.items():
            self._load_or_build_scene(scene=scene, info=info, force_rebuild=False)

    def _get_scene_storage_context(self, scene: str):
        client = chromadb.PersistentClient(path="./storage/chroma_db")
        collection = client.get_or_create_collection(f"scene_{scene}")
        vector_store = ChromaVectorStore(chroma_collection=collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        return client, vector_store, storage_context

    def _load_or_build_scene(self, scene: str, info: dict, force_rebuild: bool = False) -> None:
        """åŠ è½½æˆ–æ„å»ºå•ä¸ªåœºæ™¯ç´¢å¼•ã€‚"""
        started = time.perf_counter()
        print(f"Loading index for scene: {scene}")
        client, vector_store, storage_context = self._get_scene_storage_context(scene)
        persist_dir = f"./storage/{scene}"
        docstore_path = os.path.join(persist_dir, "docstore.json")

        if force_rebuild:
            # å¢é‡é‡å»ºæ—¶å…ˆæ¸…ç†åœºæ™¯å­˜å‚¨ï¼Œé¿å…é‡å¤å†™å…¥å‘é‡åº“ã€‚
            try:
                client.delete_collection(f"scene_{scene}")
            except Exception:
                pass
            shutil.rmtree(persist_dir, ignore_errors=True)
            _, _, storage_context = self._get_scene_storage_context(scene)
            print(f"ğŸ”„ Rebuilding index for scene: {scene}")
            index = self._build_new_index(info["path"], storage_context, persist_dir)
            self.indices[scene] = index.as_query_engine()
            elapsed = time.perf_counter() - started
            print(f"âœ… Scene {scene} rebuild finished in {elapsed:.2f}s")
            return

        if os.path.exists(persist_dir) and os.path.exists(docstore_path):
            try:
                storage_context = StorageContext.from_defaults(
                    vector_store=vector_store,
                    persist_dir=persist_dir
                )
                index = load_index_from_storage(storage_context)
                print(f"âœ… Successfully loaded existing index for scene: {scene}")
            except Exception as e:
                print(f"âš ï¸  Failed to load existing index for scene {scene}: {e}")
                print(f"ğŸ”„ Rebuilding index for scene: {scene}")
                index = self._build_new_index(info["path"], storage_context, persist_dir)
        else:
            print(f"ğŸ”„ Building new index for scene: {scene}")
            index = self._build_new_index(info["path"], storage_context, persist_dir)

        self.indices[scene] = index.as_query_engine()
        elapsed = time.perf_counter() - started
        print(f"âœ… Scene {scene} load/build finished in {elapsed:.2f}s")

    def _ensure_scene_index(self, scene: str) -> None:
        """æŒ‰éœ€ç¡®ä¿åœºæ™¯ç´¢å¼•å¯ç”¨ï¼Œé¿å…å¼•æ“åˆå§‹åŒ–æ—¶å…¨é‡åŠ è½½ã€‚"""
        if scene in self.indices:
            return
        scene_info = self.scenes.get(scene)
        if not scene_info:
            raise KeyError(f"æ— æ•ˆåœºæ™¯: {scene}")
        self._load_or_build_scene(scene=scene, info=scene_info, force_rebuild=False)

    def _build_new_index(self, data_path, storage_context, persist_dir):
        """æ„å»ºæ–°çš„ç´¢å¼•"""
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"Data path does not exist: {data_path}")

        documents = SimpleDirectoryReader(data_path).load_data()
        index = VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context
        )

        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        os.makedirs(persist_dir, exist_ok=True)
        index.storage_context.persist(persist_dir=persist_dir)
        print(f"âœ… Successfully built and persisted index for scene")

        return index

    def rebuild_scene(self, scene_key: str) -> None:
        """å¢é‡é‡å»ºæŒ‡å®šåœºæ™¯ç´¢å¼•ã€‚"""
        self.rebuild_scene_with_progress(scene_key=scene_key, on_progress=None)

    def rebuild_scene_with_progress(
        self,
        scene_key: str,
        on_progress: RebuildProgressCallback | None = None,
    ) -> None:
        """å¢é‡é‡å»ºæŒ‡å®šåœºæ™¯ç´¢å¼•ï¼ˆæ”¯æŒè¿›åº¦å›è°ƒï¼‰ã€‚"""
        self.scenes = get_scenes()
        if scene_key not in self.scenes:
            raise KeyError(f"æ— æ•ˆåœºæ™¯: {scene_key}")
        self._emit_rebuild_progress(
            on_progress=on_progress,
            stage="scene_start",
            scene_key=scene_key,
            scene_index=1,
            total_scenes=1,
            message=f"å¼€å§‹é‡å»ºåœºæ™¯: {scene_key}",
        )
        started = time.perf_counter()
        self._load_or_build_scene(scene=scene_key, info=self.scenes[scene_key], force_rebuild=True)
        self._update_scene_manifest(scene_key)
        elapsed_seconds = time.perf_counter() - started
        self._emit_rebuild_progress(
            on_progress=on_progress,
            stage="scene_done",
            scene_key=scene_key,
            scene_index=1,
            total_scenes=1,
            elapsed_seconds=elapsed_seconds,
            message=f"åœºæ™¯é‡å»ºå®Œæˆ: {scene_key}ï¼ˆ{elapsed_seconds:.2f}sï¼‰",
        )

    def rebuild_all(self) -> None:
        """é‡å»ºå‘ç”Ÿå˜åŒ–çš„åœºæ™¯ç´¢å¼•ã€‚"""
        self.rebuild_all_with_progress(on_progress=None)

    def rebuild_all_with_progress(self, on_progress: RebuildProgressCallback | None = None) -> dict[str, Any]:
        """ä»…é‡å»ºå‘ç”Ÿå˜åŒ–çš„åœºæ™¯ç´¢å¼•ï¼ˆæ”¯æŒè¿›åº¦å›è°ƒï¼‰ã€‚"""
        self.scenes = get_scenes()
        for key in list(self.indices.keys()):
            if key not in self.scenes:
                self.indices.pop(key, None)
        changed_scenes, changed_reasons = self._collect_changed_scenes()
        scene_items = [(scene_key, self.scenes[scene_key]) for scene_key in changed_scenes]
        total_scenes = len(scene_items)
        if total_scenes == 0:
            self._emit_rebuild_progress(
                on_progress=on_progress,
                stage="all_done",
                scene_index=0,
                total_scenes=0,
                message="æœªæ£€æµ‹åˆ°å˜æ›´åœºæ™¯ï¼Œå·²è·³è¿‡é‡å»º",
            )
            return {
                "changed_scene_count": 0,
                "changed_scenes": [],
                "changed_reasons": changed_reasons,
                "skipped": True,
            }
        reason_text = "ï¼›".join(
            [f"{item['scene_key']}({item['reason']})" for item in changed_reasons if item.get("scene_key") in changed_scenes]
        )
        self._emit_rebuild_progress(
            on_progress=on_progress,
            stage="all_start",
            scene_index=0,
            total_scenes=total_scenes,
            message=f"æ£€æµ‹åˆ° {total_scenes} ä¸ªå˜æ›´åœºæ™¯ï¼Œå¼€å§‹é‡å»ºï¼š{reason_text}",
        )
        for idx, (scene, info) in enumerate(scene_items, start=1):
            self._emit_rebuild_progress(
                on_progress=on_progress,
                stage="scene_start",
                scene_key=scene,
                scene_index=idx,
                total_scenes=total_scenes,
                message=f"æ­£åœ¨é‡å»ºåœºæ™¯ {idx}/{total_scenes}: {scene}",
            )
            started = time.perf_counter()
            self._load_or_build_scene(scene=scene, info=info, force_rebuild=True)
            self._update_scene_manifest(scene)
            elapsed_seconds = time.perf_counter() - started
            self._emit_rebuild_progress(
                on_progress=on_progress,
                stage="scene_done",
                scene_key=scene,
                scene_index=idx,
                total_scenes=total_scenes,
                elapsed_seconds=elapsed_seconds,
                message=f"åœºæ™¯é‡å»ºå®Œæˆ {idx}/{total_scenes}: {scene}ï¼ˆ{elapsed_seconds:.2f}sï¼‰",
            )
        self._emit_rebuild_progress(
            on_progress=on_progress,
            stage="all_done",
            scene_index=total_scenes,
            total_scenes=total_scenes,
            message=f"å˜æ›´åœºæ™¯é‡å»ºå®Œæˆï¼Œå…± {total_scenes} ä¸ªåœºæ™¯",
        )
        return {
            "changed_scene_count": total_scenes,
            "changed_scenes": changed_scenes,
            "changed_reasons": changed_reasons,
            "skipped": False,
        }

    @staticmethod
    def _emit_rebuild_progress(
        on_progress: RebuildProgressCallback | None,
        **payload: Any,
    ) -> None:
        if on_progress is None:
            return
        try:
            on_progress(payload)
        except Exception:
            # è¿›åº¦å›è°ƒä¸åº”å½±å“é‡å»ºä¸»æµç¨‹ã€‚
            pass

    def _snapshot_token_counts(self) -> dict[str, int]:
        if self._token_counter is None:
            return {
                "prompt_llm_tokens": 0,
                "completion_llm_tokens": 0,
                "total_llm_tokens": 0,
                "total_embedding_tokens": 0,
            }
        return {
            "prompt_llm_tokens": int(getattr(self._token_counter, "prompt_llm_token_count", 0) or 0),
            "completion_llm_tokens": int(getattr(self._token_counter, "completion_llm_token_count", 0) or 0),
            "total_llm_tokens": int(getattr(self._token_counter, "total_llm_token_count", 0) or 0),
            "total_embedding_tokens": int(getattr(self._token_counter, "total_embedding_token_count", 0) or 0),
        }

    @staticmethod
    def _calc_usage_delta(before: dict[str, int], after: dict[str, int]) -> dict[str, int]:
        input_tokens = max(after["prompt_llm_tokens"] - before["prompt_llm_tokens"], 0)
        output_tokens = max(after["completion_llm_tokens"] - before["completion_llm_tokens"], 0)
        total_tokens = max(after["total_llm_tokens"] - before["total_llm_tokens"], input_tokens + output_tokens)
        embedding_tokens = max(
            after["total_embedding_tokens"] - before["total_embedding_tokens"],
            0,
        )
        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "embedding_tokens": embedding_tokens,
        }

    def query_with_usage(self, user_query: str) -> dict[str, Any]:
        scene = classify_scene(user_query, runtime_mode=self.runtime_mode)
        scene_info = self.scenes.get(scene)
        if not scene_info:
            fallback_scene = get_default_scene_key()
            if fallback_scene not in self.scenes:
                fallback_scene = next(iter(self.scenes), "")
            scene = fallback_scene
            scene_info = self.scenes.get(scene, {})
            print(f"âš ï¸ æœªå‘½ä¸­æœ‰æ•ˆåœºæ™¯ï¼Œå›é€€åˆ°: {scene_info.get('name', scene)} ({scene})")
        else:
            print(f"ğŸ” è·¯ç”±åˆ°åœºæ™¯: {scene_info['name']} ({scene})")

        routed_scene = scene
        scene = self._resolve_scene_for_structured_query(scene, user_query)
        scene_info = self.scenes.get(scene, scene_info)
        structured_answer = self._build_structured_answer(scene, user_query)
        if structured_answer:
            print(f"ğŸ“Š ç»“æ„åŒ–åˆ†æµ: intent=count/list, scene={scene}")
            return {
                "answer": structured_answer,
                "answer_mode": "structured",
                "routed_scene": routed_scene,
                "answered_scene": scene,
                "usage": {
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "total_tokens": 0,
                    "embedding_tokens": 0,
                },
            }
        self._ensure_scene_index(scene)
        before = self._snapshot_token_counts()
        query_engine = self.indices[scene]
        response = query_engine.query(user_query)
        after = self._snapshot_token_counts()
        return {
            "answer": str(response),
            "answer_mode": "rag",
            "routed_scene": routed_scene,
            "answered_scene": scene,
            "usage": self._calc_usage_delta(before, after),
        }

    def query(self, user_query: str) -> str:
        return str(self.query_with_usage(user_query).get("answer", ""))
